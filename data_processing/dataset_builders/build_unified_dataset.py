#!/usr/bin/env python3
"""
ç»Ÿä¸€æ•°æ®æ ¼å¼å¤„ç†è„šæœ¬
å°†ä¸åŒframeworkçš„æ³¨å…¥å¤±è´¥æ•°æ®åˆå¹¶æˆç»Ÿä¸€æ ¼å¼çš„è®­ç»ƒé›†
"""

import os
import json
import sys
import hashlib
from pathlib import Path
from typing import Dict, List, Any, Tuple
from dataclasses import dataclass
import argparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°pathï¼Œä»¥ä¾¿å¯¼å…¥analyze_injection_errors
sys.path.append(str(Path(__file__).parent.parent))
from analyze_injection_errors import analyze_benchmark_errors, parse_filename


@dataclass
class UnifiedTrainingData:
    """ç»Ÿä¸€çš„è®­ç»ƒæ•°æ®æ ¼å¼"""
    id: str
    metadata: Dict[str, Any]
    input: Dict[str, Any]
    output: Dict[str, Any] 
    ground_truth: Dict[str, Any]


class FrameworkDataProcessor:
    """ä¸åŒæ¡†æ¶æ•°æ®å¤„ç†å™¨çš„åŸºç±»"""
    
    def __init__(self, schema_path: str):
        with open(schema_path, 'r', encoding='utf-8') as f:
            self.schema = json.load(f)
        self.agent_name_map = self.schema["agent_name_standardization"]
        self.error_type_map = self.schema["error_type_mapping"]["detailed_mapping"]
    
    def standardize_agent_name(self, original_name: str, framework: str) -> str:
        """æ ‡å‡†åŒ–æ™ºèƒ½ä½“åç§°"""
        # ç§»é™¤ç´¢å¼•å’Œç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™æ ¸å¿ƒè§’è‰²åç§°
        name = original_name.strip().lower()
        
        # é’ˆå¯¹ä¸åŒæ¡†æ¶çš„ç‰¹æ®Šå¤„ç†
        if framework == "agentverse":
            if "roleassigner" in name:
                return "RoleAssigner"
            elif "solver" in name:
                return "Solver"
            elif "critic" in name:
                return "Critic"
            elif "evaluator" in name:
                return "Evaluator"
                
        elif framework == "dylan":
            if "assistant" in name:
                return "Assistant"
            elif "tester" in name:
                return "Tester"
            elif "reflector" in name:
                return "Reflector"
            elif "programmer" in name:
                return "Programmer"
            elif "debugger" in name:
                return "Debugger"
            elif "computerscientist" in name:
                return "ComputerScientist"
            elif "algorithmdeveloper" in name:
                return "AlgorithmDeveloper"
            elif "pythonassistant" in name:
                return "PythonAssistant"
            elif "qualitymanager" in name:
                return "QualityManager"
                
        elif framework == "llm_debate":
            if "assistant" in name:
                # æå–æ•°å­—
                import re
                match = re.search(r'(\d+)', name)
                if match:
                    return f"Assistant {match.group(1)}"
                return "Assistant"
            elif "aggregator" in name:
                return "Aggregator"
            else:
                # æ·»åŠ fallbackï¼šå¦‚æœæ— æ³•è¯†åˆ«ï¼Œä½¿ç”¨é»˜è®¤åç§°
                return "Assistant"
                
        elif framework == "macnet":
            if "node" in name:
                # æå–èŠ‚ç‚¹æ ‡è¯†ï¼Œæ­£ç¡®å¤„ç†node-1è¿™æ ·çš„æƒ…å†µ
                import re
                # ä¿®å¤æ­£åˆ™è¡¨è¾¾å¼ï¼šæ­£ç¡®å¤„ç†è¿å­—ç¬¦
                match = re.search(r'node-?(\d+)', name)
                if match:
                    node_num = match.group(1)
                    # å¦‚æœåŸå§‹åç§°åŒ…å«è¿å­—ç¬¦ï¼Œä¿ç•™è¿å­—ç¬¦
                    if '-' in name:
                        return f"Node-{node_num}"
                    else:
                        return f"Node{node_num}"
                return "Node"
        
        # é»˜è®¤è¿”å›æ¸…ç†åçš„åç§°
        return original_name.title()
    
    def standardize_phase(self, original_phase: str) -> str:
        """æ ‡å‡†åŒ–å¯¹è¯é˜¶æ®µ"""
        phase = original_phase.lower()
        
        phase_map = {
            "role_assignment": "initialization",
            "solving": "reasoning", 
            "criticism": "evaluation",
            "evaluation": "evaluation",
            "discussion": "discussion",
            "decision": "decision"
        }
        
        return phase_map.get(phase, "other")
    
    def generate_id(self, metadata: Dict[str, Any], line_number: int) -> str:
        """ç”Ÿæˆå”¯ä¸€æ ‡è¯†ç¬¦"""
        source_str = f"{metadata['benchmark']}_{metadata['model']}_{metadata['framework']}_{line_number}"
        hash_obj = hashlib.md5(source_str.encode())
        return f"{source_str}_{hash_obj.hexdigest()[:8]}"
    
    def process_sample(self, sample: Dict[str, Any], file_params: Dict[str, Any], line_number: int) -> UnifiedTrainingData:
        """å¤„ç†å•ä¸ªæ ·æœ¬ï¼Œéœ€è¦åœ¨å­ç±»ä¸­å®ç°å…·ä½“é€»è¾‘"""
        raise NotImplementedError
    
    def extract_final_output(self, injection_log: Dict[str, Any]) -> str:
        """æå–ç³»ç»Ÿçš„æœ€ç»ˆè¾“å‡º"""
        final_output = injection_log.get("final_output", {})
        if isinstance(final_output, dict):
            # å°è¯•è·å–responseå­—æ®µ
            response = final_output.get("response", "")
            if response:
                return response
        elif isinstance(final_output, str):
            return final_output
        
        # å¦‚æœæ²¡æœ‰final_outputï¼Œå°è¯•ä»å…¶ä»–å­—æ®µè·å–
        if "response" in injection_log:
            return injection_log["response"]
        
        return "No final output available"


class AgentVerseProcessor(FrameworkDataProcessor):
    """AgentVerseæ¡†æ¶æ•°æ®å¤„ç†å™¨"""
    
    def parse_multi_agent_info(self, file_params: Dict[str, Any]) -> Tuple[List[str], List[str], List[str]]:
        """è§£æå¤šæ™ºèƒ½ä½“æ–‡ä»¶åä¿¡æ¯
        Returns: (agent_names, error_types, injection_strategies)
        """
        agent_type = file_params.get("agent_type", "")
        error_type = file_params.get("error_type", "")
        injection_type = file_params.get("injection_type", "")
        
        # è§£ææ™ºèƒ½ä½“åç§° - AgentVerseæ ¼å¼: solver1-evaluator1-critic2
        agent_names = []
        if agent_type:
            # æŒ‰-åˆ†å‰²ï¼Œæ¯ä¸ªéƒ¨åˆ†åŒ…å«è§’è‰²åå’Œç¼–å·
            parts = agent_type.split("-")
            for part in parts:
                if part:
                    # æå–è§’è‰²åï¼ˆå»æ‰æœ«å°¾æ•°å­—ï¼‰
                    import re
                    role_match = re.match(r'([a-zA-Z]+)\d*', part)
                    if role_match:
                        agent_names.append(role_match.group(1))
        
        # è§£æé”™è¯¯ç±»å‹ - æ ¼å¼: FM-3.2-FM-1.2-FM-2.6
        error_types = []
        if error_type:
            import re
            matches = re.findall(r'FM-\d+\.\d+', error_type)
            error_types = matches if matches else [error_type]
        
        # è§£ææ³¨å…¥ç­–ç•¥ - æ ¼å¼: response_corruption-response_corruption-prompt_injection
        injection_strategies = []
        if injection_type:
            # å…ˆç§»é™¤æœ«å°¾çš„æ•°é‡æ ‡è¯†ï¼ˆå¦‚n3ï¼‰
            import re
            cleaned_type = re.sub(r'_?n\d+$', '', injection_type)
            # æŒ‰-åˆ†å‰²
            parts = cleaned_type.split("-")
            injection_strategies = [part for part in parts if part]
        
        return agent_names, error_types, injection_strategies
    
    def process_sample(self, sample: Dict[str, Any], file_params: Dict[str, Any], line_number: int) -> UnifiedTrainingData:
        injection_log = sample.get("injection_log", {})
        
        # è§£ææ–‡ä»¶åä¸­çš„å¤šæ™ºèƒ½ä½“ä¿¡æ¯
        agent_names, error_types, injection_strategies = self.parse_multi_agent_info(file_params)
        
        # æ„å»ºmetadata
        metadata = {
            "framework": "agentverse",
            "benchmark": file_params.get("benchmark", ""),
            "model": file_params.get("model", ""),
            "num_agents": len(injection_log.get("role_descriptions", [])),
            "num_injected_agents": len(agent_names) or len(injection_log.get("multi_injection_info", [])),
            "task_type": self._infer_task_type(sample.get("tag", []))
        }
        
        # æ„å»ºè¾“å…¥ï¼ˆæ ‡å‡†åŒ–å¯¹è¯å†å²ï¼‰
        conversation_history = []
        history = injection_log.get("conversation_history", [])
        
        for i, entry in enumerate(history):
            std_entry = {
                "step": i + 1,
                "agent_name": self.standardize_agent_name(entry.get("role", ""), "agentverse"),
                "agent_role": entry.get("role", ""),
                "content": entry.get("response", ""),
                "phase": self.standardize_phase(entry.get("phase", "other"))
            }
            conversation_history.append(std_entry)
        
        input_data = {
            "query": sample.get("query", ""),
            "conversation_history": conversation_history,
            "final_output": self.extract_final_output(injection_log)
        }
        
        # æ„å»ºè¾“å‡ºï¼ˆæ•…éšœæ£€æµ‹ç»“æœï¼‰
        faulty_agents = []
        multi_injection_info = injection_log.get("multi_injection_info", [])
        
        # å¦‚æœæ—¥å¿—ä¸­æœ‰multi_injection_infoï¼Œä¼˜å…ˆä½¿ç”¨
        if multi_injection_info:
            for i, inj_info in enumerate(multi_injection_info):
                error_type = inj_info.get("fm_error_type", "")
                injection_strategy = inj_info.get("injection_strategy", "")
                
                # å¦‚æœinjection_logä¸­æ²¡æœ‰é”™è¯¯ç±»å‹ï¼Œå°è¯•ä»æ–‡ä»¶åè§£æçš„ä¿¡æ¯è·å–
                if not error_type and i < len(error_types):
                    error_type = error_types[i]
                if not injection_strategy and i < len(injection_strategies):
                    injection_strategy = injection_strategies[i]
                
                faulty_agent = {
                    "agent_name": self.standardize_agent_name(inj_info.get("injected_role", ""), "agentverse"),
                    "error_type": error_type,
                    "injection_strategy": injection_strategy
                }
                faulty_agents.append(faulty_agent)
        else:
            # ä»æ–‡ä»¶åè§£æçš„ä¿¡æ¯æ„å»ºfaulty_agents
            length = max(len(agent_names), len(error_types), len(injection_strategies), 1)
            for i in range(length):
                agent_name = agent_names[i] if i < len(agent_names) else "Unknown"
                error_type = error_types[i] if i < len(error_types) else ""
                injection_strategy = injection_strategies[i] if i < len(injection_strategies) else ""
                
                faulty_agent = {
                    "agent_name": self.standardize_agent_name(agent_name, "agentverse"),
                    "error_type": error_type,
                    "injection_strategy": injection_strategy
                }
                faulty_agents.append(faulty_agent)
        
        output_data = {
            "faulty_agents": faulty_agents
        }
        
        # æ„å»ºground_truth
        ground_truth = {
            "correct_answer": sample.get("gt", ""),
            "injected_agents": [
                {
                    "agent_name": fa["agent_name"],
                    "error_type": fa["error_type"],
                    "injection_strategy": fa["injection_strategy"],
                    "malicious_action_description": ""
                } for fa in faulty_agents
            ],
            "is_injection_successful": sample.get("status") == "success"
        }
        
        return UnifiedTrainingData(
            id=self.generate_id(metadata, line_number),
            metadata=metadata,
            input=input_data,
            output=output_data,
            ground_truth=ground_truth
        )
    
    def _infer_task_type(self, tags: List[str]) -> str:
        """æ¨æ–­ä»»åŠ¡ç±»å‹"""
        tags_str = " ".join(tags).lower()
        if "math" in tags_str:
            return "math"
        elif "code" in tags_str or "humaneval" in tags_str:
            return "code_generation"
        else:
            return "reasoning"


class DylanProcessor(FrameworkDataProcessor):
    """Dylanæ¡†æ¶æ•°æ®å¤„ç†å™¨"""
    
    def parse_multi_agent_info(self, file_params: Dict[str, Any]) -> Tuple[List[str], List[str], List[str]]:
        """è§£æå¤šæ™ºèƒ½ä½“æ–‡ä»¶åä¿¡æ¯
        Returns: (agent_names, error_types, injection_strategies)
        """
        agent_type = file_params.get("agent_type", "")
        error_type = file_params.get("error_type", "")
        injection_type = file_params.get("injection_type", "")
        
        # è§£ææ™ºèƒ½ä½“åç§° - Dylanæ ¼å¼: assistant3-assistant2-assistant4-assistant1
        agent_names = []
        if agent_type:
            # æŒ‰-åˆ†å‰²ï¼Œæ¯ä¸ªéƒ¨åˆ†åŒ…å«è§’è‰²åå’Œç¼–å·
            parts = agent_type.split("-")
            for part in parts:
                if part:
                    # ä¿ç•™å®Œæ•´çš„agentåç§°ï¼ˆåŒ…æ‹¬æ•°å­—ï¼‰
                    agent_names.append(part)
        
        # è§£æé”™è¯¯ç±»å‹ - æ ¼å¼: FM-1.4-FM-3.3-FM-2.2-FM-3.2
        error_types = []
        if error_type:
            import re
            matches = re.findall(r'FM-\d+\.\d+', error_type)
            error_types = matches if matches else [error_type]
        
        # è§£ææ³¨å…¥ç­–ç•¥ - æ ¼å¼: prompt_injection-prompt_injection-response_corruption-response_corruption
        injection_strategies = []
        if injection_type:
            # å…ˆç§»é™¤æœ«å°¾çš„æ•°é‡æ ‡è¯†ï¼ˆå¦‚n4ï¼‰
            import re
            cleaned_type = re.sub(r'_?n\d+$', '', injection_type)
            # æŒ‰-åˆ†å‰²
            parts = cleaned_type.split("-")
            injection_strategies = [part for part in parts if part]
        
        return agent_names, error_types, injection_strategies
    
    def process_sample(self, sample: Dict[str, Any], file_params: Dict[str, Any], line_number: int) -> UnifiedTrainingData:
        injection_log = sample.get("injection_log", {})
        
        # è§£ææ–‡ä»¶åä¸­çš„å¤šæ™ºèƒ½ä½“ä¿¡æ¯
        agent_names, error_types, injection_strategies = self.parse_multi_agent_info(file_params)
        
        # æ„å»ºmetadata
        metadata = {
            "framework": "dylan",
            "benchmark": file_params.get("benchmark", ""),
            "model": file_params.get("model", ""),
            "num_agents": len(set(entry.get("role", "") for entry in injection_log.get("full_history", []))),
            "num_injected_agents": len(agent_names) or 1,
            "task_type": self._infer_task_type(sample.get("tag", []))
        }
        
        # æ„å»ºè¾“å…¥ - ä¿®å¤conversation_historyä¸ºç©ºçš„é—®é¢˜
        conversation_history = []
        # ä¼˜å…ˆä½¿ç”¨conversation_historyå­—æ®µï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨full_history
        history = injection_log.get("conversation_history", [])
        if not history:
            history = injection_log.get("full_history", [])
        
        for i, entry in enumerate(history):
            # ä»entryä¸­æå–agentä¿¡æ¯
            agent_id = entry.get("agent_id", "")
            role = entry.get("role", "")
            role_index = entry.get("role_index", "")
            if role_index != "":  # ä¿®å¤ï¼šrole_indexä¸º0æ—¶ä¹Ÿéœ€è¦å¤„ç†
                role_index = int(role_index) + 1
            content = entry.get("content", entry.get("response", ""))
            
            # æ ‡å‡†åŒ–agentåç§° - ä¿ç•™åŸå§‹ç¼–å·
            if agent_id:
                # å¦‚æœagent_idåŒ…å«æ•°å­—ï¼Œä¿ç•™å®Œæ•´åç§°
                std_agent_name = agent_id.replace(" ", "")
            elif role:
                # å¦‚æœåªæœ‰roleï¼Œä½¿ç”¨role
                std_agent_name = role.replace(" ", "")
            else:
                std_agent_name = f"Assistant{i+1}"
            
            if role == "Assistant" and role_index:
                std_agent_name = f"Assistant{role_index}"
            
            std_entry = {
                "step": i + 1,
                "agent_name": std_agent_name,
                "agent_role": std_agent_name,
                "content": content,
                "phase": "reasoning"  # Dylanä¸»è¦æ˜¯æ¨ç†é˜¶æ®µ
            }
            conversation_history.append(std_entry)
        
        input_data = {
            "query": sample.get("query", ""),
            "conversation_history": conversation_history
        }
        
        # æ„å»ºè¾“å‡º - æ”¯æŒå¤šæ™ºèƒ½ä½“æ³¨å…¥ï¼Œä¿®å¤agentåç§°é—®é¢˜
        faulty_agents = []
        if agent_names:
            # ä»æ–‡ä»¶åè§£æçš„ä¿¡æ¯æ„å»ºfaulty_agents
            length = max(len(agent_names), len(error_types), len(injection_strategies), 1)
            for i in range(length):
                agent_name = agent_names[i] if i < len(agent_names) else "Unknown"
                error_type = error_types[i] if i < len(error_types) else ""
                injection_strategy = injection_strategies[i] if i < len(injection_strategies) else ""
                
                # å¯¹äºHumanEval + Dylanï¼Œå»æ‰agentåç§°æœ«å°¾çš„ç¼–å·
                if file_params.get("benchmark", "").lower() == "humaneval":
                    import re
                    # å»æ‰æœ«å°¾çš„æ•°å­—ï¼Œå¦‚ "assistant3" -> "assistant"
                    agent_name = re.sub(r'\d+$', '', agent_name)
                
                faulty_agent = {
                    "agent_name": agent_name,
                    "error_type": error_type,
                    "injection_strategy": injection_strategy
                }
                faulty_agents.append(faulty_agent)
        else:
            # å•æ™ºèƒ½ä½“æ³¨å…¥ï¼Œä½¿ç”¨æ—¥å¿—ä¸­çš„ä¿¡æ¯
            injected_role = injection_log.get("injected_role", "Unknown")
            
            # å¯¹äºHumanEval + Dylanï¼Œå»æ‰agentåç§°æœ«å°¾çš„ç¼–å·
            if file_params.get("benchmark", "").lower() == "humaneval":
                import re
                # å»æ‰æœ«å°¾çš„æ•°å­—ï¼Œå¦‚ "Assistant 1" -> "Assistant"
                injected_role = re.sub(r'\s*\d+$', '', injected_role)
            
            faulty_agent = {
                "agent_name": injected_role,
                "error_type": injection_log.get("fm_error_type", "") or "FM-2.3",
                "injection_strategy": injection_log.get("injection_strategy", "") or "prompt_injection"
            }
            faulty_agents = [faulty_agent]
        
        output_data = {
            "faulty_agents": faulty_agents
        }
        
        # æ„å»ºground_truth
        ground_truth = {
            "correct_answer": sample.get("gt", ""),
            "injected_agents": [
                {
                    "agent_name": fa["agent_name"],
                    "error_type": fa["error_type"],
                    "injection_strategy": fa["injection_strategy"],
                    "malicious_action_description": injection_log.get("malicious_action_description", "")
                } for fa in faulty_agents
            ],
            "is_injection_successful": sample.get("status") == "success"
        }
        
        return UnifiedTrainingData(
            id=self.generate_id(metadata, line_number),
            metadata=metadata,
            input=input_data,
            output=output_data,
            ground_truth=ground_truth
        )
    
    def _infer_task_type(self, tags: List[str]) -> str:
        tags_str = " ".join(tags).lower()
        if "math" in tags_str:
            return "math"
        elif "code" in tags_str or "humaneval" in tags_str:
            return "code_generation"
        else:
            return "reasoning"


class LLMDebateProcessor(FrameworkDataProcessor):
    """LLM Debateæ¡†æ¶æ•°æ®å¤„ç†å™¨"""
    
    def parse_multi_agent_info(self, file_params: Dict[str, Any]) -> Tuple[List[str], List[str], List[str]]:
        """è§£æå¤šæ™ºèƒ½ä½“æ–‡ä»¶åä¿¡æ¯
        Returns: (agent_names, error_types, injection_strategies)
        """
        agent_type = file_params.get("agent_type", "")
        error_type = file_params.get("error_type", "")
        injection_type = file_params.get("injection_type", "")
        
        # è§£ææ™ºèƒ½ä½“åç§° - LLM Debateæ ¼å¼: agent3-agent1-agent2 æˆ– agent3-aggregator4-agent1
        agent_names = []
        if agent_type:
            # æŒ‰-åˆ†å‰²ï¼Œæ¯ä¸ªéƒ¨åˆ†åŒ…å«è§’è‰²åå’Œç¼–å·
            parts = agent_type.split("-")
            agent_names = [part.replace(" ", "").lower() for part in parts]
        
        # è§£æé”™è¯¯ç±»å‹ - æ ¼å¼: FM-2.4-FM-1.1-FM-3.2
        error_types = []
        if error_type:
            import re
            matches = re.findall(r'FM-\d+\.\d+', error_type)
            error_types = matches if matches else [error_type]
        
        # è§£ææ³¨å…¥ç­–ç•¥ - æ ¼å¼: response_corruption-response_corruption-prompt_injection
        injection_strategies = []
        if injection_type:
            # å…ˆç§»é™¤æœ«å°¾çš„æ•°é‡æ ‡è¯†ï¼ˆå¦‚n3ï¼‰
            import re
            cleaned_type = re.sub(r'_?n\d+$', '', injection_type)
            # æŒ‰-åˆ†å‰²
            parts = cleaned_type.split("-")
            injection_strategies = [part for part in parts if part]
        
        return agent_names, error_types, injection_strategies
    
    def process_sample(self, sample: Dict[str, Any], file_params: Dict[str, Any], line_number: int) -> UnifiedTrainingData:
        injection_log = sample.get("injection_log", {})
        
        # è§£ææ–‡ä»¶åä¸­çš„å¤šæ™ºèƒ½ä½“ä¿¡æ¯
        agent_names, error_types, injection_strategies = self.parse_multi_agent_info(file_params)
        
        # æ„å»ºmetadata
        metadata = {
            "framework": "llm_debate",
            "benchmark": file_params.get("benchmark", ""),
            "model": file_params.get("model", ""),
            "num_agents": len(injection_log.get("agent_names", [])),
            "num_injected_agents": len(agent_names) or 1,
            "task_type": self._infer_task_type(sample.get("tag", []))
        }
        
        # æ„å»ºè¾“å…¥
        conversation_history = []
        history = injection_log.get("full_history", [])
        
        for i, entry in enumerate(history):
            # ä»entryä¸­æå–agentä¿¡æ¯
            role = entry.get("role", "").replace(" ", "")
            content = entry.get("content", "")
            
            # ä¿ç•™åŸå§‹agentåç§°ï¼Œä¸è¿›è¡Œæ ‡å‡†åŒ–
            std_agent_name = role if role else f"Agent{i+1}"
            std_agent_name = std_agent_name.replace("Agent", "Assistant")
            std_entry = {
                "step": i + 1,
                "agent_name": std_agent_name,
                "agent_role": std_agent_name,
                "content": content,
                "phase": "discussion"  # LLM Debateä¸»è¦æ˜¯è®¨è®ºé˜¶æ®µ
            }
            conversation_history.append(std_entry)
        
        input_data = {
            "query": sample.get("query", ""),
            "conversation_history": conversation_history,
            "final_output": self.extract_final_output(injection_log)
        }
        
        # æ„å»ºè¾“å‡º - æ”¯æŒå¤šæ™ºèƒ½ä½“æ³¨å…¥ï¼Œä¿®å¤agentåç§°é—®é¢˜
        faulty_agents = []
        if agent_names:
            # ä»æ–‡ä»¶åè§£æçš„ä¿¡æ¯æ„å»ºfaulty_agents
            length = max(len(agent_names), len(error_types), len(injection_strategies), 1)
            for i in range(length):
                agent_name = agent_names[i] if i < len(agent_names) else "Unknown"
                error_type = error_types[i] if i < len(error_types) else ""
                injection_strategy = injection_strategies[i] if i < len(injection_strategies) else ""
                
                if self.standardize_agent_name(agent_name, "llm_debate") == "Aggregator":
                    continue
                faulty_agent = {
                    "agent_name": agent_name.replace("agent", "Assistant"),
                    "error_type": error_type,
                    "injection_strategy": injection_strategy
                }
                faulty_agents.append(faulty_agent)
        else:
            # å•æ™ºèƒ½ä½“æ³¨å…¥ï¼Œä½¿ç”¨æ—¥å¿—ä¸­çš„ä¿¡æ¯
            error_type = injection_log.get("fm_error_type", "")
            injection_strategy = injection_log.get("injection_strategy", "")
            
            # å¦‚æœinjection_logä¸­æ²¡æœ‰ï¼Œå°è¯•ä»æ–‡ä»¶åè§£æè·å–
            if not error_type:
                error_type = file_params.get("error_type", "")
            if not injection_strategy:
                injection_strategy = file_params.get("injection_type", "")
            
            faulty_agent = {
                "agent_name": injection_log.get("injected_role", "Unknown"),
                "error_type": error_type,
                "injection_strategy": injection_strategy
            }
            faulty_agents = [faulty_agent]
        
        output_data = {
            "faulty_agents": faulty_agents
        }
        
        # æ„å»ºground_truth
        ground_truth = {
            "correct_answer": sample.get("gt", ""),
            "injected_agents": [
                {
                    "agent_name": fa["agent_name"],
                    "error_type": fa["error_type"],
                    "injection_strategy": fa["injection_strategy"],
                    "malicious_action_description": injection_log.get("malicious_action_description", "")
                } for fa in faulty_agents
            ],
            "is_injection_successful": sample.get("status") == "success"
        }
        
        return UnifiedTrainingData(
            id=self.generate_id(metadata, line_number),
            metadata=metadata,
            input=input_data,
            output=output_data,
            ground_truth=ground_truth
        )
    
    def _infer_task_type(self, tags: List[str]) -> str:
        tags_str = " ".join(tags).lower()
        if "math" in tags_str:
            return "math"
        elif "code" in tags_str or "humaneval" in tags_str:
            return "code_generation"
        else:
            return "reasoning"


class MacNetProcessor(FrameworkDataProcessor):
    """MacNetæ¡†æ¶æ•°æ®å¤„ç†å™¨"""
    
    def parse_multi_agent_info(self, file_params: Dict[str, Any]) -> Tuple[List[str], List[str], List[str]]:
        """è§£æå¤šæ™ºèƒ½ä½“æ–‡ä»¶åä¿¡æ¯
        Returns: (agent_names, error_types, injection_strategies)
        """
        agent_type = file_params.get("agent_type", "")
        error_type = file_params.get("error_type", "")
        injection_type = file_params.get("injection_type", "")
        
        # è§£ææ™ºèƒ½ä½“åç§°
        agent_names = []
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ¥æ­£ç¡®åˆ†å‰²ï¼Œå¤„ç†node-1è¿™æ ·çš„æƒ…å†µ
        import re
        # åŒ¹é…nodeåè·Ÿæ•°å­—æˆ–-æ•°å­—çš„æ¨¡å¼
        matches = re.findall(r'node-?\d+', agent_type)
        agent_names = matches
        
        # è§£æé”™è¯¯ç±»å‹
        error_types = []
        if error_type:
            if error_type.count("FM-") > 1:
                # å¤šä¸ªé”™è¯¯ç±»å‹ï¼ŒæŒ‰FM-åˆ†å‰²
                import re
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ‰¾åˆ°æ‰€æœ‰FM-X.Xæ ¼å¼çš„é”™è¯¯ç±»å‹
                matches = re.findall(r'FM-\d+\.\d+', error_type)
                error_types = matches
            else:
                error_types = [error_type]
        
        # è§£ææ³¨å…¥ç­–ç•¥
        injection_strategies = []
        if injection_type:
            # å…ˆç§»é™¤æœ«å°¾çš„æ•°é‡æ ‡è¯†ï¼ˆå¦‚n3ï¼‰
            import re
            cleaned_type = re.sub(r'_?n\d+$', '', injection_type)
            
            # æŒ‰-åˆ†å‰²ï¼Œæ¯ä¸ªéƒ¨åˆ†å°±æ˜¯ä¸€ä¸ªå®Œæ•´çš„ç­–ç•¥
            parts = cleaned_type.split("-")
            injection_strategies = [part for part in parts if part and not (part.startswith("n") and part[1:].isdigit())]
        
        return agent_names, error_types, injection_strategies
    
    def process_sample(self, sample: Dict[str, Any], file_params: Dict[str, Any], line_number: int) -> UnifiedTrainingData:
        injection_log = sample.get("injection_log", {})
        
        # è§£ææ–‡ä»¶åä¸­çš„å¤šæ™ºèƒ½ä½“ä¿¡æ¯
        agent_names, error_types, injection_strategies = self.parse_multi_agent_info(file_params)
        
        # æ„å»ºmetadata
        metadata = {
            "framework": "macnet",
            "benchmark": file_params.get("benchmark", ""),
            "model": file_params.get("model", ""),
            "num_agents": injection_log.get("total_llm_calls", len(agent_names)),
            "num_injected_agents": len(agent_names),  # ä»æ–‡ä»¶åè§£æçš„æ³¨å…¥agentæ•°é‡
            "task_type": self._infer_task_type(sample.get("tag", []))
        }
        
        # æ„å»ºè¾“å…¥
        conversation_history = []
        llm_calls = injection_log.get("llm_call_history", [])
        
        for i, call in enumerate(llm_calls):
            if not call.get("injected", False):  # åªåŒ…å«éæ³¨å…¥çš„è°ƒç”¨
                std_entry = {
                    "step": i + 1,
                    "agent_name": self.standardize_agent_name(f"node{call.get('node_id', '')}", "macnet"),
                    "agent_role": call.get("system_prompt", "")[:50],  # å–å‰50å­—ç¬¦ä½œä¸ºè§’è‰²æè¿°
                    "content": call.get("response", ""),
                    "phase": "reasoning"  # MacNetä¸»è¦æ˜¯æ¨ç†é˜¶æ®µ
                }
                conversation_history.append(std_entry)
        
        input_data = {
            "query": sample.get("query", ""),
            "conversation_history": conversation_history,
            "final_output": self.extract_final_output(injection_log)
        }
        
        # æ„å»ºè¾“å‡º - ä½¿ç”¨è§£æå‡ºçš„ä¿¡æ¯
        faulty_agents = []
        
        for i, agent_name in enumerate(agent_names):
            error_type = error_types[i] if i < len(error_types) else ""
            injection_strategy = injection_strategies[i] if i < len(injection_strategies) else ""
            
            faulty_agent = {
                "agent_name": self.standardize_agent_name(f"node{agent_name}", "macnet"),
                "error_type": error_type,
                "injection_strategy": injection_strategy
            }
            faulty_agents.append(faulty_agent)
        
        output_data = {
            "faulty_agents": faulty_agents
        }
        
        # æ„å»ºground_truth
        ground_truth = {
            "correct_answer": sample.get("gt", ""),
            "injected_agents": [
                {
                    "agent_name": self.standardize_agent_name(f"node{agent_name}", "macnet"),
                    "error_type": error_types[i] if i < len(error_types) else "",
                    "injection_strategy": injection_strategies[i] if i < len(injection_strategies) else "",
                    "malicious_action_description": f"Node {agent_name} injected with {injection_strategies[i] if i < len(injection_strategies) else 'unknown'}"
                }
                for i, agent_name in enumerate(agent_names)
            ],
            "is_injection_successful": sample.get("status") == "success"
        }
        
        return UnifiedTrainingData(
            id=self.generate_id(metadata, line_number),
            metadata=metadata,
            input=input_data,
            output=output_data,
            ground_truth=ground_truth
        )
    
    def _infer_task_type(self, tags: List[str]) -> str:
        tags_str = " ".join(tags).lower()
        if "math" in tags_str:
            return "math"
        elif "code" in tags_str or "humaneval" in tags_str:
            return "code_generation"
        else:
            return "reasoning"


class NormalSampleProcessor(FrameworkDataProcessor):
    """æ­£å¸¸æ ·æœ¬å¤„ç†å™¨ - å¤„ç†é€šè¿‡inference.pyç”Ÿæˆçš„æ­£æ ·æœ¬æ•°æ®"""
    
    def process_sample(self, sample: Dict[str, Any], file_params: Dict[str, Any], line_number: int) -> UnifiedTrainingData:
        injection_log = sample.get("injection_log", {})
        framework = injection_log.get("framework", file_params.get("framework", "unknown"))
        
        # æ„å»ºmetadata
        metadata = {
            "framework": framework,
            "benchmark": file_params.get("benchmark", ""),
            "model": file_params.get("model", "gpt-4o-mini"),
            "num_agents": self._estimate_num_agents(injection_log),
            "num_injected_agents": 0,  # æ­£æ ·æœ¬æ²¡æœ‰æ³¨å…¥
            "task_type": self._infer_task_type_from_query(sample.get("query", ""))
        }
        
        # æ„å»ºè¾“å…¥ - ä»injection_logçš„full_historyä¸­æå–
        conversation_history = []
        full_history = injection_log.get("full_history", [])
        
        # å¦‚æœfull_historyä¸ºç©ºï¼Œå°è¯•ä»å…¶ä»–å­—æ®µè·å–å†å²è®°å½•
        if not full_history:
            # å¯¹äºMacNetï¼Œå¯èƒ½æ²¡æœ‰è¯¦ç»†çš„å†å²è®°å½•ï¼Œåˆ›å»ºä¸€ä¸ªç®€åŒ–çš„æ¡ç›®
            if framework == "macnet":
                std_entry = {
                    "step": 1,
                    "agent_name": "MacNet",
                    "agent_role": "MacNet Agent",
                    "content": injection_log.get("final_output", ""),
                    "phase": "reasoning"
                }
                conversation_history.append(std_entry)
        else:
            for i, entry in enumerate(full_history):
                std_entry = {
                    "step": entry.get("step", i + 1),
                    "agent_name": self.standardize_agent_name(entry.get("agent_name", entry.get("role", "Unknown")), framework),
                    "agent_role": entry.get("role", "Unknown"),
                    "content": entry.get("content", ""),
                    "phase": self._infer_phase_from_entry(entry, framework)
                }
                conversation_history.append(std_entry)
        
        input_data = {
            "query": sample.get("query", ""),
            "conversation_history": conversation_history
        }
        
        # æ„å»ºè¾“å‡º - æ­£æ ·æœ¬æ²¡æœ‰æ•…éšœagent
        output_data = {
            "faulty_agents": []  # æ­£æ ·æœ¬æ²¡æœ‰æ•…éšœagent
        }
        
        # æ„å»ºground_truth
        response = sample.get("response", {})
        if isinstance(response, dict):
            final_response = response.get("response", str(response))
        else:
            final_response = str(response)
        
        ground_truth = {
            "correct_answer": sample.get("gt", final_response),
            "injected_agents": [],  # æ­£æ ·æœ¬æ²¡æœ‰æ³¨å…¥agent
            "is_injection_successful": False,  # æ­£æ ·æœ¬ä¸æ˜¯æ³¨å…¥å®éªŒ
            "is_normal_sample": True  # æ ‡è®°ä¸ºæ­£æ ·æœ¬
        }
        
        return UnifiedTrainingData(
            id=self.generate_id(metadata, line_number),
            metadata=metadata,
            input=input_data,
            output=output_data,
            ground_truth=ground_truth
        )
    
    def _estimate_num_agents(self, injection_log: Dict[str, Any]) -> int:
        """ä¼°ç®—æ™ºèƒ½ä½“æ•°é‡"""
        full_history = injection_log.get("full_history", [])
        if not full_history:
            return 1
        
        # ç»Ÿè®¡ä¸åŒçš„agent_nameæ•°é‡
        agent_names = set()
        for entry in full_history:
            agent_name = entry.get("agent_name", entry.get("role", "Unknown"))
            if agent_name and agent_name != "Unknown":
                agent_names.add(agent_name)
        
        return max(len(agent_names), 1)
    
    def _infer_task_type_from_query(self, query: str) -> str:
        """ä»æŸ¥è¯¢æ¨æ–­ä»»åŠ¡ç±»å‹"""
        query_lower = query.lower()
        if any(keyword in query_lower for keyword in ["math", "calculate", "solve", "equation", "number"]):
            return "math"
        elif any(keyword in query_lower for keyword in ["code", "program", "function", "implement", "algorithm"]):
            return "code_generation"
        else:
            return "reasoning"
    
    def _infer_phase_from_entry(self, entry: Dict[str, Any], framework: str) -> str:
        """ä»æ¡ç›®æ¨æ–­å¯¹è¯é˜¶æ®µ"""
        # æ ¹æ®æ¡†æ¶å’Œentryå†…å®¹æ¨æ–­é˜¶æ®µ
        if framework == "llm_debate":
            return "discussion"
        elif framework in ["dylan", "dylan_math", "dylan_humaneval", "dylan_mmlu"]:
            return "reasoning"
        elif framework in ["agentverse", "agentverse_humaneval", "agentverse_mgsm"]:
            role = entry.get("role", "").lower()
            if "role" in role or "assign" in role:
                return "initialization"
            elif "solver" in role:
                return "reasoning"
            elif "critic" in role or "eval" in role:
                return "evaluation"
            else:
                return "reasoning"
        elif framework in ["macnet", "macnet_srdd"]:
            return "reasoning"
        else:
            return "other"


class DatasetBuilder:
    """æ•°æ®é›†æ„å»ºå™¨"""
    
    def __init__(self, schema_path: str, input_dir: str, output_dir: str, normal_samples_dir: str = None, separate_datasets: bool = False, only_normal: bool = False, only_negative: bool = False):
        self.input_dir = input_dir
        self.output_dir = output_dir
        self.normal_samples_dir = normal_samples_dir  # æ­£æ ·æœ¬ç›®å½•
        self.separate_datasets = separate_datasets  # æ˜¯å¦åˆ†å¼€ä¿å­˜æ­£è´Ÿæ ·æœ¬
        self.only_normal = only_normal  # åªå¤„ç†æ­£æ ·æœ¬
        self.only_negative = only_negative  # åªå¤„ç†è´Ÿæ ·æœ¬
        
        # åˆå§‹åŒ–ä¸åŒæ¡†æ¶çš„å¤„ç†å™¨
        self.processors = {
            "agentverse": AgentVerseProcessor(schema_path),
            "dylan": DylanProcessor(schema_path),
            "llm_debate": LLMDebateProcessor(schema_path),
            "macnet": MacNetProcessor(schema_path)
        }
        
        # æ­£æ ·æœ¬å¤„ç†å™¨
        self.normal_processor = NormalSampleProcessor(schema_path)
        
        os.makedirs(output_dir, exist_ok=True)
    
    def get_framework_from_filename(self, filename: str) -> str:
        """ä»æ–‡ä»¶åæ¨æ–­æ¡†æ¶ç±»å‹"""
        if filename.startswith("agentverse"):
            return "agentverse"
        elif filename.startswith("dylan"):
            return "dylan" 
        elif filename.startswith("llm_debate"):
            return "llm_debate"
        elif filename.startswith("macnet"):
            return "macnet"
        else:
            return "unknown"
    
    def build_dataset(self):
        """æ„å»ºç»Ÿä¸€æ ¼å¼çš„æ•°æ®é›†"""
        print("ğŸš€ å¼€å§‹æ„å»ºç»Ÿä¸€æ ¼å¼çš„è®­ç»ƒæ•°æ®é›†...")
        
        # ä½¿ç”¨analyze_injection_errorsè·å–ç­”é¢˜é”™è¯¯çš„æ•°æ®
        all_benchmark_errors = {}
        
        if not self.only_normal:  # åªæœ‰åœ¨ä¸æ˜¯åªå¤„ç†æ­£æ ·æœ¬æ—¶æ‰åˆ†ææ³¨å…¥é”™è¯¯æ•°æ®
            print("ğŸ“Š åˆ†ææ³¨å…¥å®éªŒé”™è¯¯æ•°æ®...")
            
            # è·å–æ‰€æœ‰benchmarkç›®å½•
            benchmark_dirs = [d for d in os.listdir(self.input_dir) if
                                os.path.isdir(os.path.join(self.input_dir, d)) and d !=
                                "smoagents_logs"]
            
            for benchmark in benchmark_dirs:
                benchmark_path = os.path.join(self.input_dir, benchmark)
                print(f"  å¤„ç†benchmark: {benchmark}")
                
                benchmark_result = analyze_benchmark_errors(benchmark_path, benchmark)
                all_benchmark_errors[benchmark] = benchmark_result
        else:
            print("â­ï¸ è·³è¿‡æ³¨å…¥é”™è¯¯æ•°æ®åˆ†æï¼ˆ--only_normal æ¨¡å¼ï¼‰")
        
        # å¤„ç†ç­”é¢˜é”™è¯¯çš„æ•°æ®ï¼ˆè¿™äº›æ˜¯æˆ‘ä»¬è¦çš„è®­ç»ƒæ•°æ®ï¼‰
        negative_samples = []
        total_processed = 0
        
        if not self.only_normal:  # å¦‚æœä¸æ˜¯åªå¤„ç†æ­£æ ·æœ¬ï¼Œåˆ™å¤„ç†è´Ÿæ ·æœ¬
            for benchmark, result in all_benchmark_errors.items():
                answer_errors = result['answer_errors']  # åªè¦ç­”é¢˜é”™è¯¯çš„æ•°æ®
                print(f"  ğŸ“ {benchmark}: æ‰¾åˆ° {len(answer_errors)} ä¸ªç­”é¢˜é”™è¯¯æ ·æœ¬")
                
                for error_sample in answer_errors:
                    try:
                        file_params = error_sample.get('file_params', {})
                        framework = file_params.get('framework', '')
                        
                        if framework in self.processors:
                            processor = self.processors[framework]
                            
                            # å¤„ç†æ ·æœ¬
                            unified_sample = processor.process_sample(
                                error_sample, 
                                file_params, 
                                error_sample.get('line_number', 0)
                            )
                            
                            negative_samples.append(unified_sample)
                            total_processed += 1
                            
                            if total_processed % 100 == 0:
                                print(f"    å·²å¤„ç† {total_processed} ä¸ªè´Ÿæ ·æœ¬...")
                                
                    except Exception as e:
                        print(f"    âš ï¸ å¤„ç†æ ·æœ¬æ—¶å‡ºé”™: {e}")
                        continue
            
            print(f"âœ… è´Ÿæ ·æœ¬å¤„ç†å®Œæˆï¼æ€»å…±å¤„ç†äº† {total_processed} ä¸ªè´Ÿæ ·æœ¬")
        else:
            print("â­ï¸ è·³è¿‡è´Ÿæ ·æœ¬å¤„ç†ï¼ˆ--only_normal æ¨¡å¼ï¼‰")
        
        # å¤„ç†æ­£æ ·æœ¬ï¼ˆå¦‚æœæä¾›äº†æ­£æ ·æœ¬ç›®å½•ï¼‰
        positive_samples = []
        if not self.only_negative and self.normal_samples_dir and os.path.exists(self.normal_samples_dir):
            print(f"\nğŸ“‹ å¤„ç†æ­£æ ·æœ¬æ•°æ®...")
            normal_count = self._process_normal_samples(positive_samples)
            print(f"âœ… æ­£æ ·æœ¬å¤„ç†å®Œæˆï¼æ€»å…±å¤„ç†äº† {normal_count} ä¸ªæ­£æ ·æœ¬")
        elif self.only_negative:
            print("â­ï¸ è·³è¿‡æ­£æ ·æœ¬å¤„ç†ï¼ˆ--only_negative æ¨¡å¼ï¼‰")
        elif not self.normal_samples_dir:
            print("âš ï¸ æœªæä¾›æ­£æ ·æœ¬ç›®å½•ï¼Œè·³è¿‡æ­£æ ·æœ¬å¤„ç†")
        
        # æ ¹æ®è®¾ç½®ä¿å­˜æ•°æ®é›†
        if self.separate_datasets:
            print(f"ğŸ¯ åˆ†åˆ«ä¿å­˜æ­£è´Ÿæ ·æœ¬æ•°æ®é›†...")
            if negative_samples:
                print(f"  ğŸ’¾ ä¿å­˜è´Ÿæ ·æœ¬æ•°æ®é›† ({len(negative_samples)} ä¸ªæ ·æœ¬)")
                self.save_dataset(negative_samples, suffix="negative")
            if positive_samples:
                print(f"  ğŸ’¾ ä¿å­˜æ­£æ ·æœ¬æ•°æ®é›† ({len(positive_samples)} ä¸ªæ ·æœ¬)")  
                self.save_dataset(positive_samples, suffix="positive")
        else:
            # åˆå¹¶ä¿å­˜
            unified_data = negative_samples + positive_samples
            print(f"ğŸ¯ æ•°æ®é›†æ„å»ºå®Œæˆï¼æ€»è®¡ {len(unified_data)} ä¸ªæ ·æœ¬")
            self.save_dataset(unified_data)
    
    def _process_normal_samples(self, positive_samples: List[UnifiedTrainingData]) -> int:
        """å¤„ç†æ­£æ ·æœ¬æ•°æ®"""
        normal_count = 0
        
        # éå†æ­£æ ·æœ¬ç›®å½•
        for root, dirs, files in os.walk(self.normal_samples_dir):
            for file in files:
                if file.endswith('.jsonl'):
                    file_path = os.path.join(root, file)
                    
                    # ä»æ–‡ä»¶è·¯å¾„æ¨æ–­frameworkå’Œbenchmark
                    rel_path = os.path.relpath(file_path, self.normal_samples_dir)
                    path_parts = rel_path.split(os.sep)
                    
                    # å‡è®¾è·¯å¾„æ ¼å¼ä¸º: benchmark/model/framework/filename.jsonl
                    if len(path_parts) >= 3:
                        benchmark = path_parts[0]
                        model = path_parts[1] 
                        framework_part = path_parts[2]
                        
                        # ä»æ–‡ä»¶åæ¨æ–­æ¡†æ¶
                        framework = self.get_framework_from_filename(file)
                        if framework == "unknown":
                            framework = framework_part
                        
                        file_params = {
                            "framework": framework,
                            "benchmark": benchmark,
                            "model": model,
                            "filename": file
                        }
                        
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                for line_num, line in enumerate(f, 1):
                                    if line.strip():
                                        try:
                                            sample = json.loads(line)
                                            
                                            # æ£€æŸ¥æ˜¯å¦æ˜¯æˆåŠŸçš„æ ·æœ¬ä¸”åŒ…å«injection_log
                                            if (sample.get("status") == "success" and 
                                                "injection_log" in sample):
                                                
                                                unified_sample = self.normal_processor.process_sample(
                                                    sample, file_params, line_num
                                                )
                                                positive_samples.append(unified_sample)
                                                normal_count += 1
                                                
                                                if normal_count % 100 == 0:
                                                    print(f"    å·²å¤„ç† {normal_count} ä¸ªæ­£æ ·æœ¬...")
                                        
                                        except json.JSONDecodeError:
                                            continue
                        
                        except Exception as e:
                            print(f"    âš ï¸ å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
                            continue
        
        return normal_count
    
    def save_dataset(self, unified_data: List[UnifiedTrainingData], suffix: str = ""):
        """ä¿å­˜æ•°æ®é›†åˆ°ä¸åŒæ ¼å¼"""
        print("ğŸ’¾ ä¿å­˜æ•°æ®é›†...")
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        data_dicts = []
        for item in unified_data:
            data_dict = {
                "id": item.id,
                "metadata": item.metadata,
                "input": item.input,
                "output": item.output,
                "ground_truth": item.ground_truth
            }
            data_dicts.append(data_dict)
        
        # æ ¹æ®åç¼€ç”Ÿæˆæ–‡ä»¶å
        if suffix:
            base_name = f"unified_training_dataset_{suffix}"
        else:
            base_name = "unified_training_dataset_easy"
        
        # ä¿å­˜ä¸ºJSONLæ ¼å¼ï¼ˆé€‚åˆå¤§æ•°æ®é›†ï¼‰
        jsonl_path = os.path.join(self.output_dir, f"{base_name}.jsonl")
        with open(jsonl_path, 'w', encoding='utf-8') as f:
            for data_dict in data_dicts:
                f.write(json.dumps(data_dict, ensure_ascii=False) + '\n')
        
        # ä¿å­˜ä¸ºJSONæ ¼å¼ï¼ˆæ–¹ä¾¿æŸ¥çœ‹ï¼‰
        json_path = os.path.join(self.output_dir, f"{base_name}.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(data_dicts, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆæ•°æ®é›†ç»Ÿè®¡æŠ¥å‘Š
        stats_path = self.generate_statistics(data_dicts, suffix)
        
        print(f"âœ… æ•°æ®é›†å·²ä¿å­˜åˆ°:")
        print(f"  ğŸ“„ JSONLæ ¼å¼: {jsonl_path}")
        print(f"  ğŸ“„ JSONæ ¼å¼: {json_path}")
        if stats_path:
            print(f"  ğŸ“Š ç»Ÿè®¡æŠ¥å‘Š: {stats_path}")
    
    def generate_statistics(self, data_dicts: List[Dict[str, Any]], suffix: str = ""):
        """ç”Ÿæˆæ•°æ®é›†ç»Ÿè®¡æŠ¥å‘Š"""
        print("ğŸ“Š ç”Ÿæˆæ•°æ®é›†ç»Ÿè®¡æŠ¥å‘Š...")
        
        stats = {
            "æ€»æ ·æœ¬æ•°": len(data_dicts),
            "æŒ‰æ¡†æ¶åˆ†å¸ƒ": {},
            "æŒ‰æ•°æ®é›†åˆ†å¸ƒ": {},
            "æŒ‰ä»»åŠ¡ç±»å‹åˆ†å¸ƒ": {},
            "æŒ‰é”™è¯¯ç±»å‹åˆ†å¸ƒ": {},
            "æŒ‰æ³¨å…¥ç­–ç•¥åˆ†å¸ƒ": {},
            "å¤šæ™ºèƒ½ä½“æ³¨å…¥åˆ†å¸ƒ": {}
        }
        
        for data in data_dicts:
            metadata = data["metadata"]
            output = data["output"]
            
            # æ¡†æ¶åˆ†å¸ƒ
            framework = metadata["framework"]
            stats["æŒ‰æ¡†æ¶åˆ†å¸ƒ"][framework] = stats["æŒ‰æ¡†æ¶åˆ†å¸ƒ"].get(framework, 0) + 1
            
            # æ•°æ®é›†åˆ†å¸ƒ
            benchmark = metadata["benchmark"]
            stats["æŒ‰æ•°æ®é›†åˆ†å¸ƒ"][benchmark] = stats["æŒ‰æ•°æ®é›†åˆ†å¸ƒ"].get(benchmark, 0) + 1
            
            # ä»»åŠ¡ç±»å‹åˆ†å¸ƒ
            task_type = metadata["task_type"]
            stats["æŒ‰ä»»åŠ¡ç±»å‹åˆ†å¸ƒ"][task_type] = stats["æŒ‰ä»»åŠ¡ç±»å‹åˆ†å¸ƒ"].get(task_type, 0) + 1
            
            # é”™è¯¯ç±»å‹åˆ†å¸ƒ
            for agent in output["faulty_agents"]:
                error_type = agent["error_type"]
                stats["æŒ‰é”™è¯¯ç±»å‹åˆ†å¸ƒ"][error_type] = stats["æŒ‰é”™è¯¯ç±»å‹åˆ†å¸ƒ"].get(error_type, 0) + 1
                
                # æ³¨å…¥ç­–ç•¥åˆ†å¸ƒ
                strategy = agent["injection_strategy"]
                stats["æŒ‰æ³¨å…¥ç­–ç•¥åˆ†å¸ƒ"][strategy] = stats["æŒ‰æ³¨å…¥ç­–ç•¥åˆ†å¸ƒ"].get(strategy, 0) + 1
            
            # å¤šæ™ºèƒ½ä½“æ³¨å…¥åˆ†å¸ƒ
            num_injected = metadata["num_injected_agents"]
            stats["å¤šæ™ºèƒ½ä½“æ³¨å…¥åˆ†å¸ƒ"][str(num_injected)] = stats["å¤šæ™ºèƒ½ä½“æ³¨å…¥åˆ†å¸ƒ"].get(str(num_injected), 0) + 1
        
        # æ ¹æ®åç¼€ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Šæ–‡ä»¶å
        if suffix:
            stats_filename = f"dataset_statistics_{suffix}.json"
        else:
            stats_filename = "dataset_statistics_easy.json"
        
        # ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
        stats_path = os.path.join(self.output_dir, stats_filename)
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°ç»Ÿè®¡æ‘˜è¦
        print("\nğŸ“ˆ æ•°æ®é›†ç»Ÿè®¡æ‘˜è¦:")
        print(f"  æ€»æ ·æœ¬æ•°: {stats['æ€»æ ·æœ¬æ•°']}")
        print(f"  æ¡†æ¶åˆ†å¸ƒ: {stats['æŒ‰æ¡†æ¶åˆ†å¸ƒ']}")
        print(f"  æ•°æ®é›†åˆ†å¸ƒ: {stats['æŒ‰æ•°æ®é›†åˆ†å¸ƒ']}")
        print(f"  ä»»åŠ¡ç±»å‹åˆ†å¸ƒ: {stats['æŒ‰ä»»åŠ¡ç±»å‹åˆ†å¸ƒ']}")
        if stats['æŒ‰é”™è¯¯ç±»å‹åˆ†å¸ƒ']:
            print(f"  é”™è¯¯ç±»å‹åˆ†å¸ƒ (å‰5): {dict(list(sorted(stats['æŒ‰é”™è¯¯ç±»å‹åˆ†å¸ƒ'].items(), key=lambda x: x[1], reverse=True))[:5])}")
        
        return stats_path


def main():
    parser = argparse.ArgumentParser(description="æ„å»ºç»Ÿä¸€æ ¼å¼çš„è®­ç»ƒæ•°æ®é›†")
    parser.add_argument("--input_dir", default="results_inj", help="è¾“å…¥ç›®å½•ï¼ˆåŒ…å«æ³¨å…¥å®éªŒç»“æœï¼‰")
    parser.add_argument("--output_dir", default="data_processing/unified_dataset", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--schema_path", default="data_processing/unified_schema.json", help="ç»Ÿä¸€æ ¼å¼schemaæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--normal_samples_dir", default=None, help="æ­£æ ·æœ¬æ•°æ®ç›®å½•ï¼ˆåŒ…å«é€šè¿‡inference.pyç”Ÿæˆçš„æ­£å¸¸æ¨ç†ç»“æœï¼‰")
    parser.add_argument("--separate", action="store_true", help="åˆ†åˆ«ä¿å­˜æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬åˆ°ä¸åŒæ–‡ä»¶")
    parser.add_argument("--only_normal", action="store_true", help="åªå¤„ç†æ­£æ ·æœ¬ï¼Œè·³è¿‡è´Ÿæ ·æœ¬å¤„ç†")
    parser.add_argument("--only_negative", action="store_true", help="åªå¤„ç†è´Ÿæ ·æœ¬ï¼Œè·³è¿‡æ­£æ ·æœ¬å¤„ç†")
    
    args = parser.parse_args()
    
    # æ„å»ºæ•°æ®é›†
    args.normal_samples_dir = "results_right"
    # args.only_normal = True
    args.separate = True
    builder = DatasetBuilder(args.schema_path, args.input_dir, args.output_dir, args.normal_samples_dir, args.separate, args.only_normal, args.only_negative)
    builder.build_dataset()
    
    print("ğŸ‰ ç»Ÿä¸€æ ¼å¼è®­ç»ƒæ•°æ®é›†æ„å»ºå®Œæˆï¼")


if __name__ == "__main__":
    main()