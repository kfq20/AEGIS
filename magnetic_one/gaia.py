import asyncio
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.teams import MagenticOneGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.agents.web_surfer import MultimodalWebSurfer
from autogen_ext.agents.file_surfer import FileSurfer
from autogen_ext.agents.magentic_one import MagenticOneCoderAgent
from autogen_agentchat.agents import CodeExecutorAgent
from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor
from typing import Any, Dict, List, Literal, Optional, Union, Tuple, Callable
from injection import MagenticInjectionCoordinator
import random
import json
import argparse
import os

parser = argparse.ArgumentParser()
parser.add_argument("--inject", default=True, help="Enable malicious injection.")
parser.add_argument("--target-agent", type=str, default="WebSurfer", choices=["Orchestrator", "WebSurfer", "FileSurfer", "Coder", "ComputerTerminal"], help="The agent to inject.")
parser.add_argument("--fm-type", type=str, default="FM-2.5", help="The FM error type to inject.")
parser.add_argument("--injection-strategy", type=str, choices=["prompt_injection", "response_corruption"], default="prompt_injection", help="The injection strategy.")
# æ·»åŠ å­é›†ç›¸å…³å‚æ•°
parser.add_argument("--randomize", action="store_true", help="éšæœºåŒ–ä»»åŠ¡é¡ºåº")
parser.add_argument("--level", type=str, default="all", help="GAIA level: 1, 2, 3, æˆ– 'all' è¡¨ç¤ºæ‰€æœ‰çº§åˆ«")
parser.add_argument("--on", type=str, default="valid", choices=["valid", "test"], help="æ•°æ®é›†ç±»å‹")
# æ—¥å¿—æ•è·ç›¸å…³å‚æ•°
parser.add_argument("--capture-mode", type=str, default="stream", 
                    choices=["stream", "memory", "direct", "none"],
                    help="æ—¥å¿—æ•è·æ¨¡å¼: stream(æµå¼ç¼“å­˜,æ¨è), memory(å†…å­˜ä¼˜åŒ–), direct(ç›´æ¥æ–‡ä»¶), none(ä¸æ•è·)")
parser.add_argument("--flush-threshold", type=int, default=8192, help="æµå¼æ¨¡å¼çš„ç¼“å­˜é˜ˆå€¼(å­—èŠ‚)")
parser.add_argument("--limit", type=int, default=None, help="é™åˆ¶å¤„ç†çš„ä»»åŠ¡æ•°é‡ï¼Œç”¨äºæ‰¹é‡æ•°æ®æ”¶é›†")


def create_model_client():
        return OpenAIChatCompletionClient(
            model="gpt-4o-mini-2024-07-18",
            api_key=os.getenv("OPENAI_API_KEY", "YOUR_API_KEY_HERE"),  # Use environment variable
            base_url=os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")  # Use environment variable
        )

def load(force_download=False):
        r"""Load the GAIA dataset.

        Args:
            force_download (bool, optional): Whether to
                force download the data.
        """
        # Define validation and test directories - use relative paths or environment variables
        valid_dir = os.getenv("GAIA_VALIDATION_DIR", "./data/gaia/2023/validation")
        test_dir = os.getenv("GAIA_TEST_DIR", "./data/gaia/2023/test")

        all_data = {}
        # Load metadata for both validation and test datasets
        for path, label in zip([valid_dir, test_dir], ["valid", "test"]):
            all_data[label] = []
            with open(f"{path}/metadata.jsonl", "r") as f:
                lines = f.readlines()
                for line in lines:
                    data = json.loads(line)
                    if data["task_id"] == "0-0-0-0-0":
                        continue
                    if data["file_name"]:
                        data["file_name"] = f"{path}/{data['file_name']}"
                    all_data[label].append(data)
        return all_data

def load_tasks(
        on: Literal["valid", "test"],
        level: Union[int, List[int], Literal["all"]],
        randomize: bool = False,
        idx: Optional[List[int]] = None,
        use_subset: bool = False,
        subset_file: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        r"""Load tasks from the dataset."""
        gaia_data = load()
        if on not in ["valid", "test"]:
            raise ValueError(
                f"Invalid value for `on`: {on}, expected 'valid' or 'test'."
            )
        levels = (
            [1, 2, 3]
            if level == "all"
            else [level]
            if isinstance(level, int)
            else level
        )   
        
        datas = [data for data in gaia_data[on] if data["Level"] in levels]
        
        # å¦‚æœä½¿ç”¨å­é›†ï¼Œè¿‡æ»¤ä»»åŠ¡
        if use_subset and subset_file:
            if os.path.exists(subset_file):
                with open(subset_file, 'r', encoding='utf-8') as f:
                    subset_data = json.load(f)
                    subset_task_ids = set(subset_data.get("task_ids", []))
                    print(f"ğŸ“‹ ä½¿ç”¨å­é›†æ•°æ®é›†: {subset_file}")
                    print(f"ğŸ“Š å­é›†åŒ…å« {len(subset_task_ids)} ä¸ªä»»åŠ¡")
                    
                    # è¿‡æ»¤æ•°æ®
                    original_count = len(datas)
                    datas = [data for data in datas if data["task_id"] in subset_task_ids]
                    print(f"ğŸ” è¿‡æ»¤åå‰©ä½™ {len(datas)} ä¸ªä»»åŠ¡ (ä» {original_count} ä¸ª)")
            else:
                print(f"âš ï¸  å­é›†æ–‡ä»¶ä¸å­˜åœ¨: {subset_file}")
        
        if randomize:
            random.shuffle(datas)
        
        if idx is not None:
            # pick only the tasks with the specified idx
            if len(idx) != 0:   
                datas = [datas[i] for i in idx]
                
        return datas

async def run_task_and_capture_log(team, task_prompt, capture_mode="stream", max_log_size=None):
    """
    ä¼˜åŒ–çš„æ—¥å¿—æ•è·å‡½æ•° - ä¿è¯å®Œæ•´è¾“å‡ºçš„åŒæ—¶ä¼˜åŒ–æ€§èƒ½
    
    Args:
        capture_mode: 
            - "stream": æµå¼å†™å…¥æ–‡ä»¶ï¼Œä¿æŒå®Œæ•´æ—¥å¿—(æ¨è)
            - "memory": å†…å­˜ç¼“å­˜ï¼Œå¸¦æ€§èƒ½ä¼˜åŒ–
            - "direct": ç›´æ¥æ–‡ä»¶å†™å…¥ï¼Œæœ€èŠ‚çœå†…å­˜
            - "none": ä¸æ•è·æ—¥å¿—
    """
    import sys
    import gc
    import tempfile
    import os
    import threading
    from collections import deque
    
    if capture_mode == "none":
        print(f"Running task: {task_prompt}")
        await Console(team.run_stream(task=task_prompt))
        return f"Task completed: {task_prompt[:100]}..."
    
    elif capture_mode == "direct":
        # ç›´æ¥å†™å…¥æ–‡ä»¶ï¼Œå†…å­˜å ç”¨æœ€å°
        with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.log', encoding='utf-8') as temp_file:
            old_stdout = sys.stdout
            sys.stdout = temp_file
            
            try:
                print(f"Running task: {task_prompt}")
                await Console(team.run_stream(task=task_prompt))
            finally:
                sys.stdout = old_stdout
                temp_file.flush()
                
                # è¯»å–å®Œæ•´æ–‡ä»¶å†…å®¹
                with open(temp_file.name, 'r', encoding='utf-8') as f:
                    log_content = f.read()
                
                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                os.unlink(temp_file.name)
                gc.collect()
        
        return log_content
    
    elif capture_mode == "stream":
        # æµå¼ç¼“å­˜ï¼Œå®šæœŸåˆ·æ–°åˆ°ç£ç›˜ï¼Œå¹³è¡¡æ€§èƒ½å’Œå®Œæ•´æ€§
        class StreamingFileBuffer:
            def __init__(self, flush_threshold=8192):  # 8KB ç¼“å­˜
                self.temp_file = tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.log', encoding='utf-8')
                self.buffer = []
                self.buffer_size = 0
                self.flush_threshold = flush_threshold
                self.lock = threading.Lock()
            
            def write(self, s):
                with self.lock:
                    self.buffer.append(s)
                    self.buffer_size += len(s)
                    
                    # è¾¾åˆ°é˜ˆå€¼æ—¶åˆ·æ–°åˆ°æ–‡ä»¶
                    if self.buffer_size >= self.flush_threshold:
                        self._flush_buffer()
                
                return len(s)
            
            def _flush_buffer(self):
                if self.buffer:
                    self.temp_file.write(''.join(self.buffer))
                    self.temp_file.flush()
                    self.buffer.clear()
                    self.buffer_size = 0
            
            def flush(self):
                with self.lock:
                    self._flush_buffer()
                    self.temp_file.flush()
            
            def getvalue(self):
                with self.lock:
                    self._flush_buffer()
                    self.temp_file.seek(0)
                    content = self.temp_file.read()
                    self.temp_file.close()
                    
                    # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                    try:
                        os.unlink(self.temp_file.name)
                    except:
                        pass
                    
                    return content

        old_stdout = sys.stdout
        stream_buffer = StreamingFileBuffer()
        sys.stdout = stream_buffer
        
        try:
            print(f"Running task: {task_prompt}")
            await Console(team.run_stream(task=task_prompt))
        finally:
            sys.stdout = old_stdout
            log_content = stream_buffer.getvalue()
            gc.collect()
        
        return log_content
    
    else:  # capture_mode == "memory"
        # ä¼˜åŒ–çš„å†…å­˜æ•è·ï¼Œä½¿ç”¨ deque æé«˜æ€§èƒ½
        class OptimizedStringIO:
            def __init__(self):
                self.chunks = deque()
                self.total_size = 0
            
            def write(self, s):
                if s:
                    self.chunks.append(s)
                    self.total_size += len(s)
                return len(s)
            
            def getvalue(self):
                if not self.chunks:
                    return ""
                result = ''.join(self.chunks)
                # æ¸…ç†å†…å­˜
                self.chunks.clear()
                self.total_size = 0
                return result
            
            def flush(self):
                pass

        old_stdout = sys.stdout
        mystdout = OptimizedStringIO()
        sys.stdout = mystdout
        
        try:
            print(f"Running task: {task_prompt}")
            await Console(team.run_stream(task=task_prompt))
        finally:
            sys.stdout = old_stdout
            log_content = mystdout.getvalue()
            del mystdout  # æ˜¾å¼åˆ é™¤
            gc.collect()
        
        return log_content

async def main() -> None:
    args = parser.parse_args()

    model_client = OpenAIChatCompletionClient(
        model="gpt-4o-mini-2024-07-18",
        api_key=os.getenv("OPENAI_API_KEY", "YOUR_OPENAI_API_KEY_HERE"),
        base_url=os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
    )
    injection_coordinator = MagenticInjectionCoordinator(args, model_client)
    # print(f"ğŸ” DEBUG: injection_coordinator created")
    # print(f"ğŸ” DEBUG: injection_coordinator.enabled = {injection_coordinator.enabled}")
    # print(f"ğŸ” DEBUG: injection_coordinator.target_agent_name = {injection_coordinator.target_agent_name}")
    # print(f"ğŸ” DEBUG: injection_coordinator.fm_error_type = {injection_coordinator.fm_error_type}")
    # print(f"ğŸ” DEBUG: injection_coordinator.injection_strategy = {injection_coordinator.injection_strategy}")
    
    # å¤„ç†çº§åˆ«å‚æ•°
    if args.level == "all":
        LEVEL = "all"
        level_for_loading = "all"
        subset_file_path = os.getenv("GAIA_SUBSET_DIR", "./data/gaia_subset") + "/gaia_subset_all_levels.json"
    else:
        try:
            LEVEL = int(args.level)
            level_for_loading = LEVEL
            subset_file_path = os.getenv("GAIA_SUBSET_DIR", "./data/gaia_subset") + f"/gaia_subset_level_{args.level}.json"
        except ValueError:
            raise ValueError(f"æ— æ•ˆçš„çº§åˆ«å‚æ•°: {args.level}. è¯·ä½¿ç”¨ 1, 2, 3 æˆ– 'all'")
    
    on = args.on
    OUTPUT_PATH = f"magentic_one/injection_logs/level_{LEVEL}_{on}_{args.target_agent}_{args.fm_type}_{args.injection_strategy}.json"

    # åŠ è½½ä»»åŠ¡
    tasks = load_tasks(
        on=on, 
        level=level_for_loading,
        randomize=args.randomize,
        use_subset=True,
        subset_file=subset_file_path
    )
    
    print(f"ğŸ“‹ åŠ è½½äº† {len(tasks)} ä¸ªä»»åŠ¡")
    if len(tasks) > 0:
        print(f"ğŸ” ç¬¬ä¸€ä¸ªä»»åŠ¡ID: {tasks[0]['task_id']}")

    # === æ–­ç‚¹é‡ç»­é€»è¾‘ä¿æŒä¸å˜ ===
    if os.path.exists(OUTPUT_PATH):
        with open(OUTPUT_PATH, "r", encoding="utf-8") as f:
            all_logs = json.load(f)
    else:
        all_logs = {}

    model_client = OpenAIChatCompletionClient(
        model="gpt-4o-mini-2024-07-18",
        api_key=os.getenv("OPENAI_API_KEY", "YOUR_OPENAI_API_KEY_HERE"),
        base_url=os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
    )

    # === ä¿®æ”¹ï¼šæ¯ä¸ªä»»åŠ¡åˆ›å»ºæ–°çš„å›¢é˜Ÿ ===
    completed_count = 0
    for task in tasks:
        task_id = task["task_id"]
        if task_id in all_logs:
            print(f"Task {task_id} å·²å®Œæˆï¼Œè·³è¿‡ã€‚")
            continue
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é™åˆ¶
        if args.limit is not None and completed_count >= args.limit:
            print(f"ğŸ¯ å·²å®Œæˆ {completed_count} ä¸ªä»»åŠ¡ï¼Œè¾¾åˆ°é™åˆ¶ ({args.limit})ï¼Œåœæ­¢å¤„ç†ã€‚")
            break
        
        surfer_client = create_model_client()
        file_surfer_client = create_model_client()
        coder_client = create_model_client()
        team_client = create_model_client()
        
        # ä¸ºæ¯ä¸ªä»»åŠ¡åˆ›å»ºæ–°çš„å›¢é˜Ÿå®ä¾‹
        surfer = MultimodalWebSurfer("WebSurfer", model_client=surfer_client)
        file_surfer = FileSurfer("FileSurfer", model_client=file_surfer_client)
        coder = MagenticOneCoderAgent("Coder", model_client=coder_client)
        terminal = CodeExecutorAgent("ComputerTerminal", code_executor=LocalCommandLineCodeExecutor())
        team = MagenticOneGroupChat([surfer, file_surfer, coder, terminal], model_client=team_client)
        
        # print(f"ğŸ” DEBUG: Created team for task {task_id}")
        # print(f"ğŸ” DEBUG: team.model_client id = {id(team._model_client) if hasattr(team, '_model_client') else 'NO_MODEL_CLIENT'}")
        # print(f"ğŸ” DEBUG: injection model_client id = {id(model_client)}")
        
        # åº”ç”¨æ³¨å…¥ï¼ˆå¦‚æœç›®æ ‡ä¸æ˜¯Orchestratorï¼Œç«‹å³æ³¨å…¥ï¼›å¦åˆ™å»¶è¿Ÿæ³¨å…¥ï¼‰
        injection_coordinator.apply_injection(team)
        
        # å¦‚æœç›®æ ‡agentæ˜¯Orchestratorï¼Œéœ€è¦åœ¨è¿è¡Œæ—¶æ³¨å…¥
        if injection_coordinator.enabled and injection_coordinator.target_agent_name == "Orchestrator":
            # åˆ›å»ºä¸€ä¸ªåŒ…è£…å‡½æ•°æ¥åœ¨è¿è¡Œæ—¶æ³¨å…¥
            original_run_stream = team.run_stream
            
            async def run_stream_with_injection(*args, **kwargs):
                # print("ğŸ” DEBUG: run_stream_with_injection called!")
                # print(f"ğŸ” DEBUG: hasattr(team, '_group_chat_manager') = {hasattr(team, '_group_chat_manager')}")
                # print(f"ğŸ” DEBUG: team._group_chat_manager = {getattr(team, '_group_chat_manager', 'NOT_FOUND')}")
                
                # ç¡®ä¿åœ¨æ¯æ¬¡è¿è¡Œæ—¶æ³¨å…¥éƒ½æ˜¯æ´»è·ƒçš„
                # print("ğŸ¯ Runtime injection: Applying injection to ensure all LLM calls are intercepted...")
                injection_coordinator.apply_injection(team)
                # è¿”å›å¼‚æ­¥ç”Ÿæˆå™¨ï¼Œè€Œä¸æ˜¯åç¨‹
                async for message in original_run_stream(*args, **kwargs):
                    yield message
            team.run_stream = run_stream_with_injection

        question = task["Question"]
        answer = task["Final answer"]
        all_logs[task_id] = {}
        all_logs[task_id]["question"] = question
        all_logs[task_id]["correct_answer"] = answer
        all_logs[task_id]["logs"] = await run_task_and_capture_log(
            team, question, 
            capture_mode=args.capture_mode
        )

        # æ¯åšå®Œä¸€ä¸ªå°±ä¿å­˜ä¸€æ¬¡
        os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
        with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
            json.dump(all_logs, f, ensure_ascii=False, indent=2)
        
        completed_count += 1
        print(f"Task {task_id} å®Œæˆ ({completed_count}/{args.limit if args.limit else len(tasks)})")

    print(f"å…¨éƒ¨ä»»åŠ¡å¤„ç†å®Œæˆï¼Œæ—¥å¿—å·²ä¿å­˜åˆ° {OUTPUT_PATH}")

if __name__ == "__main__":
    asyncio.run(main())